{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbdwxvFIekQ1"
      },
      "source": [
        "# Object recognition pipeline\n",
        "\n",
        "The pipeline is based on few models/frameworks:\n",
        "\n",
        "\n",
        "**BLIP-2** - image captioning ([paper](https://arxiv.org/pdf/2301.12597), [HuggingFace](https://huggingface.co/docs/transformers/model_doc/blip-2)).\n",
        "\n",
        "\n",
        "**spaCY** - english text analyser (https://spacy.io/), see [dependency parsing](https://spacy.io/usage/linguistic-features#dependency-parse).\n",
        "\n",
        "\n",
        "**GroundingDINO/SAM** - open set object detection and segmentation ([SAM official site](https://segment-anything.com/), [SAM demo](https://segment-anything.com/demo#), [SAM Github](https://github.com/facebookresearch/segment-anything), [GroundingDINO Github](https://github.com/IDEA-Research/GroundingDINO), [HF Grounding DINO demo](https://huggingface.co/spaces/merve/Grounding_DINO_demo), [GroundingSAM GitHub](https://github.com/IDEA-Research/Grounded-Segment-Anything), [GroundingSAM example in Colab](https://colab.research.google.com/github/betogaona7/Grounded-Segment-Anything/blob/main/grounded_sam_colab_demo.ipynb)).\n",
        "\n",
        "**LLaVA** - general purpose multimodal model that was learned by Chat-GPT-3.5 to solve visual understanding tasks ([official repo](https://github.com/haotian-liu/LLaVA), [demo](https://llava-vl.github.io/)).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Set-up environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgaqAI5ZeSHx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUmIGVgTeq6c"
      },
      "source": [
        "## Load an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWOBTLKXellL"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/ant-nik/semares/master/data/stereo-camera-cyl/image68_r.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
        "display(image.resize((596, 437)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLIP 2"
      ],
      "metadata": {
        "id": "Mqq47IvMQuOb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS1zpJMqfVH2"
      },
      "source": [
        "### Loading BLIP 2 model\n",
        "\n",
        "There are model and checkpoints in HuggingFace.The model and its processor can be found at [hub](https://huggingface.co/models?other=blip-2). Also it is require d to load a checkpoint (pre-trained OPT model by Meta AI, which as 2.7 billion parameters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ebas399gfUIF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "# optimize RAM by using float16\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwliJCpqZdVF"
      },
      "source": [
        "A GPU improves a performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDjYgnSIDF91"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2JrGIO19-hH"
      },
      "source": [
        "### List objects on an image with BLIP 2\n",
        "\n",
        "No prompt is required if we only want to captionize an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24jOWISkfeHA"
      },
      "outputs": [],
      "source": [
        "prompt = \"Question: What objects are in the image? Answer:\"\n",
        "\n",
        "inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples of a context (a chat-like dialogue) - describe objects from previous step."
      ],
      "metadata": {
        "id": "j5Rr5pKfIAsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # \"a ball, a checkerboard, a person, and a ball.\"\n",
        "context = f\"{prompt} {generated_text}\"\n",
        "chat_prompt = f\"{context}. Question: Which color is a person's pants? Answer:\"\n",
        "inputs = processor(image, text=chat_prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "-YdeeHeME_tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GroundingSAM"
      ],
      "metadata": {
        "id": "d60Ap6oTRorg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLaVa\n",
        "\n",
        "[GitHub](https://github.com/haotian-liu/LLaVA), [Demo](https://llava-vl.github.io/)"
      ],
      "metadata": {
        "id": "CcCWgdi4Q44O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ant-nik/LLaVA.git"
      ],
      "metadata": {
        "id": "BHBIo34QQ6zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate"
      ],
      "metadata": {
        "id": "KqaLyv2QT1HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LLaVA"
      ],
      "metadata": {
        "id": "fHwa61Z1T-82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir offload"
      ],
      "metadata": {
        "id": "XbCt85ctZ0mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -e ."
      ],
      "metadata": {
        "id": "dXp2I7iYUH5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llava.model.builder import load_pretrained_model\n",
        "from llava.mm_utils import get_model_name_from_path\n",
        "from llava.eval.run_llava import eval_model\n",
        "\n",
        "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
        "\n",
        "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
        "    model_path=model_path,\n",
        "    model_base=None,\n",
        "    model_name=get_model_name_from_path(model_path)\n",
        ")"
      ],
      "metadata": {
        "id": "KlFOmJ_SRTHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
        "prompt = \"Provide bounding boxes for objects.\"\n",
        "image_file = \"https://raw.githubusercontent.com/ant-nik/semares/master/data/stereo-camera-cyl/image68_r.jpg\"\n",
        "\n",
        "args = type('Args', (), {\n",
        "    \"model_path\": model_path,\n",
        "    \"model_base\": None,\n",
        "    \"model_name\": get_model_name_from_path(model_path),\n",
        "    \"query\": prompt,\n",
        "    \"conv_mode\": None,\n",
        "    \"image_file\": image_file,\n",
        "    \"sep\": \",\",\n",
        "    \"temperature\": 0,\n",
        "    \"top_p\": None,\n",
        "    \"num_beams\": 1,\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"offload_folder\": \"./offload\"\n",
        "})()\n",
        "\n",
        "eval_model(args)"
      ],
      "metadata": {
        "id": "TbQSGluXVMDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GGn9H5s6Z9M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic segmentation\n",
        "\n",
        "[demo](https://replicate.com/cjwbw/semantic-segment-anything)"
      ],
      "metadata": {
        "id": "jtQafVzJS2oF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Segment Anything\n",
        "\n",
        "[SSA Github](https://github.com/fudan-zvg/Semantic-Segment-Anything)\n",
        "\n",
        "[SSA Demo](https://replicate.com/cjwbw/semantic-segment-anything)"
      ],
      "metadata": {
        "id": "N1kuvT1xZmbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAM -> BLIP 2 tool\n",
        "\n",
        "[Colab example](https://colab.research.google.com/github/ttengwang/Caption-Anything/blob/main/notebooks/tutorial.ipynb)"
      ],
      "metadata": {
        "id": "j8q6DHfVdh4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAM based annotation\n",
        "\n",
        "[SegDrawer home page](https://github.com/lujiazho/SegDrawer)\n",
        "\n",
        "[SegDrawer in colab with ngrok proxy](https://github.com/lujiazho/SegDrawer/blob/main/SegDrawer.ipynb)"
      ],
      "metadata": {
        "id": "xjiC6Tg1bwqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## spyCY\n",
        "\n",
        "Text dependencies parsing is required to find, for example, list of objects (nouns) and their dependencies to parse models output.\n",
        "\n",
        "See [installation](https://github.com/explosion/spaCy?tab=readme-ov-file#-install-spacy), [model loading](https://github.com/explosion/spaCy?tab=readme-ov-file#-install-spacy) and [dependency parsing](https://spacy.io/usage/linguistic-features#dependency-parse).\n"
      ],
      "metadata": {
        "id": "N23ZWca2SpPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "P4Y5ZY3kSqC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "04DgVK3NTY08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import en_core_web_sm\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "doc = nlp(\"This is a sentence.\")"
      ],
      "metadata": {
        "id": "o6elQ64QT0Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "\n",
        "table = {\n",
        "    \"text\": [],  \"dep\": [], \"head_text\": [], \"head_pos\": [],\n",
        "    \"children\": []\n",
        "}\n",
        "for token in doc:\n",
        "    table[\"text\"].append(token.text)\n",
        "    table[\"head_text\"].append(token.head.text)\n",
        "    table[\"head_pos\"].append(token.head.pos_)\n",
        "    table[\"dep\"].append(token.dep_)\n",
        "    table[\"children\"].append([child for child in token.children])\n",
        "\n",
        "table = pandas.DataFrame(table)\n",
        "table"
      ],
      "metadata": {
        "id": "wniDKyzUSu5K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}